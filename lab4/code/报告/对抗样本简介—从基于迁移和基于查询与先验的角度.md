# 对抗样本简介—从基于迁移和基于查询与先验的角度

所有的代码位于https://github.com/deadfffool/ADV，这是我之前学习对抗样本时候的实验仓库

# Abstract

如今，深度学习已被广泛应用于图像分类和图像识别的问题中，取得了令人满意的实际效果，成为许多人工智能应用的关键所在.在对于模型准确率的不断探究中，研究人员在近期提出了“对抗样本”这一概念.通过在原有样本中添加微小扰动的方法，成功地大幅度降低原有分类深度模型的准确率，实现了对于深度学习的对抗目的，同时也给深度学习的攻方提供了新的思路，对如何开展防御提出了新的要求.在介绍对抗样本生成技术的起源和原理的基础上，对近年来有关对抗样本的研究和文献进行了总结，主要集中在基于迁移和基于查询的角度。

# Introduction

随着深度学习的快速发展与巨大成功，深度学习被应用在许多对安全有严格要求的环境中。然而，深度神经网络近来被发现，对于精心设计好的输入样本，其是脆弱的，这种样本就被称为**对抗样本**。对抗样本对人类是很容易分辨的，但却能在测试或部署阶段，很容易的糊弄深度神经网络。当应用深度神经网络到对安全有严格要求的环境中时，处理对抗样本造成的脆弱性变成已成了一个重要的任务。因此对抗样本的**攻击**和**防御**吸引了很大的注意。

对抗样本由 Christian Szegedy 等人提出，是指在数据集中通过故意添加细微的干扰所形成的输入样本，这种样本导致模型以高置信度给出一个错误的输出。在正则化背景下，通过对抗训练减少原有独立同分布的测试集的错误率，在对抗扰动的训练集样本上训练网络。

# Preliminaries

简单地讲，对抗样本通过在原始数据上叠加精心构造的人类难以察觉的扰动，使深度学习模型产生分类错误。以图像分类模型为例，如图所示，通过在原始图像上叠加扰动，对于肉眼来说，扰动非常细微，图像看起来还是能猫，但是图像分类模型却会以很大的概率识别为长臂猿。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled.png)

下面以一个图像分类模型为例，更加直接地解释对抗样本的基本原理。通过在训练样本上学习，学到一个分割平面，在分割平面一侧的为绿球，在分割平面另外一侧的为红球。生成攻击样本的过程，就是在数据上添加一定的扰动，让其跨越分割平面，从而把分割平面一侧的红球识别为绿球，如下图所示。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%201.png)

对抗样本按照攻击后的效果分为 Targeted Attack (定性攻击)和 Non-Targeted Attack(无定向攻击)。区别在于 Targeted Attack 在攻击前会设置攻击的目标，比如把红球识别为绿球，或者把面包识别为熊猫，也就是说在攻击后的效果是确定的; Non-Targeted Attack在攻击前不用设置攻击目标，只要攻击后，识别的结果发生改变即可，可能会把面包识别为熊猫，也可能识别为小猪佩琪或者小猪乔治，如图所示。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%202.png)

对抗样本按照攻击成本分为 White-Box Attack(白盒攻击)Black-Box Attack(黑盒攻击)和 Real-World Attack/PhysicalAttack (真实世界/物理攻击)。White-Box Attack是其中攻击难度最低的一种，前提是能够完整获取模型的结构，包括模型的组成以及隔层的参数情况，并且可以完整控制模型的输入，对输人的控制粒度甚至可以到比特级别。由于 White-Box Attack 前置条件过于苛刻，通常作为实验室的学术研究或者作为发起 Black-Box Attack的基础。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%203.png)

Black-Box Attack 相对 White-Box Attack 攻击难度具有很大提高，Black-Box Attack完全把被攻击模型当成一个黑盒，对模型的结构没有了解，只能控制输入，通过比对输入和输出的反馈来进行下一步攻击，如下图所示。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%204.png)

# Adversarial Examples

## Basic Method

### **FGM/FGSM**

论文为 Explaining and Harnessing Adversarial Examples [https://arxiv.org/abs/1412.6572v3](https://arxiv.org/abs/1412.6572v3)

FGM也被称作FGSM，快速梯度算法，fast gradient method，可以作为无定向攻击和定向攻击算法使用。假设图片原始数据为$x$，图片识别的结果为$y$，原始图像叠加上的细微变化为$η$，肉眼难以识别，公式如下$\widetilde{x} = x + η$

将修改的图像传入模型之后会与参数矩阵和激活函数相作用，我们的目标是追求微小的修改来对分类的结果产生变化，因此我们采用$sign$函数，将变化量与梯度的方向相一致，就可以对分类结果产生较大的变化。当$x$的维度为$n$时，模型在每个维度的平均值为$m$，$η$的无穷范数为$ε$，每个维度的微小改变都与函数梯度的方向一致，累计的效果就为$n*m*ε$，当数据的维度很大时，即使$η$很小，对最后结果的影响也可能很大。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%205.png)

### **DeepFool**

论文为 CVPR 2016 DeepFool: a simple and accurate method to fool deep neural networks

Deepfool的思想就是利用迭代的方式一步步向着分类的决策边界移动，为了找到最小的决策边界，我们利用while 循环找出最小扰动可以到达的边界，然后通过找出扰动方向，利用梯度不断向边界靠近，实现分类的攻击

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%206.png)

### **JSMA**

论文为 IEEE 2016 The Limitations of Deep Learning in Adversarial Settings [https://arxiv.org/pdf/1511.07528.pdf](https://arxiv.org/pdf/1511.07528.pdf)

JSMA引入了显著图(Saliency Map)的概念，该算法致力于用扰动较少的像素点来完成定向攻击，所以从Saliency Map中查找需要扰动的像素点

Saliency Map的生成方式为对原始标签和其他标签求梯度，找到是原始标签损失上升而其他标签损失下降最快的点，并执行迭代更新

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%207.png)

### **C&W攻击算法**

论文为 Towards Evaluating the Robustness of Neural Networks [https://arxiv.org/pdf/1608.04644.pdf](https://arxiv.org/pdf/1608.04644.pdf)

CW通常被认为是攻击能力最强的白盒攻击算法之一，达到了但是的SOTA，是一种基于优化的算法，CW算法的论文打破的防御蒸馏这种对抗防御的方法，CW的零感来在于原来的Box-Constrained L-BFGS，论文中的优化目标如下图所示

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%208.png)

作者尝试了不同的loss，并测试了他们的表现，发现f6在实验中表现最好，在后续许多攻击中也采用f6这种loss

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%209.png)

关于box constraints，作者利用变量变换，引入新的变量w，将对抗样本表示为下图所示，这样子可以有效保障我们的优化不会溢出且具有良好的梯度。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2010.png)

最后的优化目标如下（采用l2范数攻击）

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2011.png)

### **PGD攻击算法**

PGD证明了明确了对抗样本需要解决的问题，将其归结于一个最大最小化问题

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2012.png)

这个公式给了一种统一的视角，把这类鞍点问题看作内部最大化和外部最小化问题的组合，内部问题利用给定数据点x来找到一个具有高损失的对抗样本，外部最小化问题是找到合适的模型参数，使内部攻击模型的损失最小化。
PGD攻击的方式为

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2013.png)

PGD的特点之一就在于它在Constrain内随机重启，作者发现在对抗样本生成空间内有很多局部的最优解，FGSM类算法并没有完全捕获到攻击空间的丰富度，随机重启可以找到所有空间内的一阶对抗样本（他们基本上是正交的，也没有极端的异常值）
通过Danskin‘s theorem可以知道该鞍点中内部最大化的方向恰好也是外部最小化的方向，我们将对抗样本加入到训练集中，便可以训练出更加鲁棒的神经网络。

## Black-Box Attacks using ****Transferable**** Adversarial Examples

### **MI-FGSM攻击算法**

论文为 Boosting Adversarial Attacks with Momentum

MI-FGSM 将动量相关的梯度整合进对抗样本的迭代过程中，在训练的过程中，使用动量法可以有效的稳定更新的方向，跳出局部极值，使得对抗样本获得更好的迁移性。

MI-FGSM也可以运用到集成攻击中，进一步提高迁移性

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2014.png)

### **NI-FGSM攻击算法**

论文来自我们学校的何琨老师 ICLR 2020 Nesterov accelerated gradient and scale invariance for adversarial attacks 

如果说MI-FGSM借鉴了梯度下降中的momentum算法，NI-FGSM便是借鉴了Nesterov Accelerated Gradient（NAG）算法，该算法的公式如下。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2015.png)

该算法的想法很简单，在momentum项中，我们会利用以前的梯度，那么既然已经知道一定会走$\alpha * \beta * d_{i-1}$,何必还要用原来那一个点的梯度，直接利用走一步之后那个点的梯度。在相关的数学分析后，我们会发现这个操作实际上是利用了部分二阶导数的信息，来达到稳定梯度更新方向的效果，达到更好的收敛效果，即NI-FGSM比MI-FGSM具有更好的前瞻性

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2016.png)

### **VM（N）I-FGSM攻击算法**

同样来自何琨老师，CVPR 2021 Enhancing the Transferability of Adversarial Attacks through Variance Tuning

VMI-FGSM不再直接使用前一步的梯度进行梯度累计，而是进一步考虑前一步迭代的梯度方差来调整当前梯度，从而稳定梯度方向。
梯度方差为

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2017.png)

由于输入空间的连续性，我们无法计算周围空间的数学期望，所以采用sample的方式，在N的样本里取平均值达到期望的无偏估计的效果

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2018.png)

VMI-FGSM受方差缩减方法的启发，这类方法可以有效稳定更新的梯度之间的方差，稳定更新方向，更快更好得达到极值点。经典的SAG和SVRG算法的核心思想就是通过采样构建当前梯度的无偏估计，加速收敛。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2019.png)

### 通过数据增强的算法来提高迁移性

思路是通过input transformation来进行数据增强，利用一定的先验知识来找到更好的对抗样本。

相关的方法有
Diverse Input Method (DIM)  Random resizing and padding

Translation-Invariant Method (TIM)

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2020.png)

Scale-Invariant Method (SIM)

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2021.png)

Admix Attack Method (Admix)

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2022.png)

## Black-Box Adversarial Attacks with Priors

### **NES算法 NATURAL EVOLUTIONARY STRATEGIES**

论文为Black-box Adversarial Attacks with Limited Queries and Information

在真实情况下，查询的次数是有限的，作者利用自然演化算法来估计梯度，其中采样的方法是高斯采样分布，而且是对称的，这样出来的是无偏估计，且上界下界不断逼近。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2023.png)

### **Partial-Information下的NES算法**
****

Partial-Information setting下，攻击者只能访问top k种分类的概率，甚至无法获得一个准确的softmax分布。
这时算法从目标分类直接出发，投影到x的范围内，进行自然演替，在更新样本的过程中不断调整超参数。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2024.png)

### **Label-Only下的NES算法**
****

更极端的情况下，我们甚至无法获得分数，只能获得一个top k分类，此时我们利用随机扰动生成对应输出概率的代理

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2025.png)

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2026.png)

### **Priors**

论文为Prior Convictions:Black-Box Adversarial Attacks with Bandits and Priors

1. **Time-dependent Priors** 作者在实验中发现迭代的步之间梯度是高度相关的，可以将t-1步的梯度作为t步梯度的先验。

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2027.png)

1. **Data-dependent Priors** 在图像分类的情况下，图像往往具有空间相似性，在两个非常接近的像素点的位置，梯度是十分相近的

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2028.png)

![Untitled](%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E7%AE%80%E4%BB%8B%E2%80%94%E4%BB%8E%E5%9F%BA%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9F%A5%E8%AF%A2%E4%B8%8E%E5%85%88%E9%AA%8C%E7%9A%84%E8%A7%92%E5%BA%A6%20e9a8b6a999af4246a6dbb3f5eb71a4e7/Untitled%2029.png)

# 参考文献

## **经典的基于梯度的黑盒攻击算法**

1. Explaining and Harnessing Adversarial Examples
2. DeepFool: a simple and accurate method to fool deep neural networks
3. The Limitations of Deep Learning in Adversarial Settings
4. Towards Evaluating the Robustness of Neural Networks
5. Towards Deep Learning Models Resistant to Adversarial Attacks

## **提高黑盒迁移性的相关算法**

1. Boosting Adversarial Attacks with Momentum
2. Nesterov accelerated gradient and scale invariance for adversarial attacks
3. Enhancing the Transferability of Adversarial Attacks through Variance Tuning
4. Enhancing the Transferability of Adversarial Attacks through Variance Tuning
5. Admix Enhancing the Transferability of Adversarial Attacks
6. Improving Transferability of Adversarial Examples With Input Diversity
7. Evading Defenses to Transferable Adversarial Examples byt Translation-Invariant Attacks

## **基于查询和先验知识的黑盒攻击算法**

1. Natural evolution strategies
2. Black-box Adversarial Attacks with Limited Queries and Information
3. Learning Black-Box Attackers with Transferable Priors and Query Feedback
4. Prior Convictions:Black-Box Adversarial Attacks with Bandits and Priors